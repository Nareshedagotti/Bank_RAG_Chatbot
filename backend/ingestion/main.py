"""
Banking RAG Ingestion Pipeline â€” Entry Point
Runs the full LangGraph pipeline to extract, chunk, embed,
and store banking PDFs into Milvus.
"""

import argparse
import logging
import sys
import time
from pathlib import Path

from ingestion.config import PDF_DIR, PROCESSED_DIR


def _setup_logging(verbose: bool = False) -> None:
    """Configure structured logging."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(name)s â”‚ %(message)s",
        datefmt="%H:%M:%S",
        handlers=[
            logging.StreamHandler(sys.stdout),
        ],
    )
    # Suppress noisy libraries
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
    logging.getLogger("pymilvus").setLevel(logging.WARNING)


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Banking RAG Ingestion Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                    # Run with defaults
  python main.py --pdf-dir ./pdfs   # Custom PDF directory
  python main.py --verbose          # Debug logging
        """,
    )
    parser.add_argument(
        "--pdf-dir",
        type=str,
        default=None,
        help=f"PDF directory (default: {PDF_DIR})",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable debug logging",
    )
    args = parser.parse_args()

    _setup_logging(args.verbose)
    logger = logging.getLogger(__name__)

    pdf_dir = Path(args.pdf_dir) if args.pdf_dir else PDF_DIR

    if not pdf_dir.exists():
        logger.error(f"PDF directory not found: {pdf_dir}")
        sys.exit(1)

    pdf_count = len(list(pdf_dir.glob("*.pdf")))
    if pdf_count == 0:
        logger.error(f"No PDF files found in {pdf_dir}")
        sys.exit(1)

    # Ensure output directory exists
    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

    logger.info(f"PDF directory : {pdf_dir} ({pdf_count} files)")
    logger.info(f"Output dir    : {PROCESSED_DIR}")

    # Run pipeline
    from ingestion.pipeline.pipeline import run_pipeline

    start = time.time()
    final_state = run_pipeline(pdf_dir)
    elapsed = time.time() - start

    # Summary
    stats = final_state.get("stats", {})
    errors = final_state.get("errors", [])

    print("\n" + "â•" * 50)
    print("  PIPELINE SUMMARY")
    print("â•" * 50)

    if stats.get("skipped"):
        print("  â­  No new/changed files â€” skipped")
    else:
        print(f"  ğŸ“„ Documents   : {stats.get('total_documents', '?')}")
        print(f"  ğŸ“ƒ Pages       : {stats.get('total_pages_extracted', '?')}")
        print(f"  ğŸ§© Chunks      : {stats.get('total_chunks', '?')}")
        print(f"  ğŸ“Š Table chunks: {stats.get('chunks_with_tables', '?')}")
        print(f"  ğŸ”¢ Embeddings  : {stats.get('embeddings_generated', '?')}")
        print(f"  ğŸ’¾ Chroma recs : {stats.get('chroma_records_stored', '?')}")
        print(f"  â±  Time        : {elapsed:.1f}s")

    if errors:
        print(f"\n  âš  Errors ({len(errors)}):")
        for err in errors:
            print(f"    â€¢ {err}")

    print("â•" * 50 + "\n")

    sys.exit(1 if errors else 0)


if __name__ == "__main__":
    main()
